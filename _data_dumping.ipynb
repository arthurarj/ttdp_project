{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/utils_data_cleaning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df = load_taxi_data_chunk(chunk=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump it to a binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pkl.dump(df, open('file.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read it back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = pkl.load(open('file.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Loading the data using pickle, from a binary file, is much faster than doing from the .csv file directly.\n",
    "\n",
    "Just delete the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('file.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Generate pkl files from csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original csv file has 131165043 (**~130M**) rows. With a chunkesiz of **50k**, we should **2624 chunks**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2624"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 50000\n",
    "chunk_number = int(131165043 / chunk_size + 1)\n",
    "chunk_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that it takes 25 seconds to load one chunk with that size, the total would take **18 hours**. \n",
    "\n",
    "To avoid that for now, let's load a smaller number of chunks (e.g. 10). We can load more later using the parameter *skiprows*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_to_skip = 626\n",
    "rows_to_skip = chunks_to_skip * chunk_size\n",
    "\n",
    "start = chunks_to_skip + 1\n",
    "\n",
    "# How many chunks to load (-1 for all)\n",
    "chunks_to_load = -1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 627/2624 \n",
      "Size reduction from 17353 to 16932 (421 samples dropped for missing data)\n",
      "Size reduction from 16932 to 16905 (27 samples dropped for having longer duration than 7200 seconds)\n",
      "Size reduction from 16905 to 16891 (14 samples dropped for having outside the region of interest)\n",
      "Size reduction from 16891 to 16861 (30 samples dropped for being invalid)\n",
      "Final chunk size is 16861 (492 dropped).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run utils/utils_data_cleaning.py\n",
    "\n",
    "logging.basicConfig(filename='_data_dumping.log',\n",
    "                    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "                    datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "i = start\n",
    "for data_chunk in pd.read_csv('../data/2016_Yellow_Taxi_Trip_Data.csv', chunksize=chunk_size, skiprows=range(1, rows_to_skip + 1), parse_dates=date_columns):\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"\\rChunk {0}/{1} \".format(i, chunk_number))    \n",
    "    \n",
    "    data_chunk.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    data_chunk.rename(columns_to_rename,axis=1, inplace=True)\n",
    "    \n",
    "    # Handle Missing Data\n",
    "    prior_size = data_chunk.shape[0]\n",
    "    data_chunk = handle_missing_data(data_chunk)\n",
    "    \n",
    "    # Augment data\n",
    "    data_chunk['duration'] = data_chunk.apply(lambda r: (r['do_t'] - r['pu_t']).seconds, axis=1)\n",
    "    data_chunk['vec_dist'] = data_chunk.apply(lambda s : geopy.distance.geodesic((s.pu_lat, s.pu_lon),(s.do_lat, s.do_lon)).miles, axis=1)\n",
    "    data_chunk['trip_ratio'] = data_chunk.trip_dist / data_chunk.vec_dist\n",
    "    \n",
    "    # Handle outliers and invalid trips\n",
    "    handle_duration_outliers(data_chunk, 7200)\n",
    "    handle_spatial_outliers(data_chunk)\n",
    "    handle_invalid_trips(data_chunk)\n",
    "    \n",
    "    # Dump it\n",
    "    pkl.dump(data_chunk, open('../data/bin_chunks/ttd_chunk_{0}.pkl'.format(i), 'wb'))\n",
    "\n",
    "    print(\"Final chunk size is {0} ({1} dropped).\\n\".format(data_chunk.shape[0], prior_size - data_chunk.shape[0]), end='\\n')\n",
    "\n",
    "    logging.info(\"Chunk {0}: Size went from {1} to {2}\".format(i, prior_size, data_chunk.shape[0]))\n",
    "    \n",
    "    i = i + 1\n",
    "    if chunks_to_load is not -1:\n",
    "        if i - start + 1 > chunks_to_load:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test one of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pu_t</th>\n",
       "      <th>do_t</th>\n",
       "      <th>trip_dist</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>duration</th>\n",
       "      <th>vec_dist</th>\n",
       "      <th>trip_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24950000</th>\n",
       "      <td>2016-03-16 00:42:57</td>\n",
       "      <td>2016-03-16 00:57:12</td>\n",
       "      <td>4.60</td>\n",
       "      <td>-74.000359</td>\n",
       "      <td>40.727055</td>\n",
       "      <td>-73.944664</td>\n",
       "      <td>40.691372</td>\n",
       "      <td>855</td>\n",
       "      <td>3.822906</td>\n",
       "      <td>1.203273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24950001</th>\n",
       "      <td>2016-03-07 13:45:13</td>\n",
       "      <td>2016-03-07 14:02:16</td>\n",
       "      <td>2.03</td>\n",
       "      <td>-73.987152</td>\n",
       "      <td>40.756435</td>\n",
       "      <td>-73.982407</td>\n",
       "      <td>40.736088</td>\n",
       "      <td>1023</td>\n",
       "      <td>1.425953</td>\n",
       "      <td>1.423609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24950002</th>\n",
       "      <td>2016-03-10 17:34:17</td>\n",
       "      <td>2016-03-10 17:45:44</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-73.963234</td>\n",
       "      <td>40.768913</td>\n",
       "      <td>-73.973885</td>\n",
       "      <td>40.761600</td>\n",
       "      <td>687</td>\n",
       "      <td>0.752898</td>\n",
       "      <td>1.474303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24950003</th>\n",
       "      <td>2016-03-07 07:00:55</td>\n",
       "      <td>2016-03-07 07:19:18</td>\n",
       "      <td>6.60</td>\n",
       "      <td>-73.986320</td>\n",
       "      <td>40.767590</td>\n",
       "      <td>-73.944054</td>\n",
       "      <td>40.841949</td>\n",
       "      <td>1103</td>\n",
       "      <td>5.589228</td>\n",
       "      <td>1.180843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24950004</th>\n",
       "      <td>2016-03-27 12:30:35</td>\n",
       "      <td>2016-03-27 12:34:20</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-73.952156</td>\n",
       "      <td>40.771736</td>\n",
       "      <td>-73.952454</td>\n",
       "      <td>40.766136</td>\n",
       "      <td>225</td>\n",
       "      <td>0.386730</td>\n",
       "      <td>1.292892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        pu_t                do_t  trip_dist     pu_lon  \\\n",
       "24950000 2016-03-16 00:42:57 2016-03-16 00:57:12       4.60 -74.000359   \n",
       "24950001 2016-03-07 13:45:13 2016-03-07 14:02:16       2.03 -73.987152   \n",
       "24950002 2016-03-10 17:34:17 2016-03-10 17:45:44       1.11 -73.963234   \n",
       "24950003 2016-03-07 07:00:55 2016-03-07 07:19:18       6.60 -73.986320   \n",
       "24950004 2016-03-27 12:30:35 2016-03-27 12:34:20       0.50 -73.952156   \n",
       "\n",
       "             pu_lat     do_lon     do_lat  duration  vec_dist  trip_ratio  \n",
       "24950000  40.727055 -73.944664  40.691372       855  3.822906    1.203273  \n",
       "24950001  40.756435 -73.982407  40.736088      1023  1.425953    1.423609  \n",
       "24950002  40.768913 -73.973885  40.761600       687  0.752898    1.474303  \n",
       "24950003  40.767590 -73.944054  40.841949      1103  5.589228    1.180843  \n",
       "24950004  40.771736 -73.952454  40.766136       225  0.386730    1.292892  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pkl.load(open('../data/bin_chunks/ttd_chunk_500.pkl', 'rb'))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine enough chunks to make a 10M size dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chunks = 200\n",
    "chunks = [pkl.load(open('../data/bin_chunks/ttd_chunk_{0}.pkl'.format(i), 'rb')) for i in range(1, n_chunks + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.concat(chunks,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Data shape:\", dataset.shape)\n",
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_10M = dataset[:10000000]\n",
    "dataset_10M = dataset_10M.reset_index()\n",
    "pkl.dump(dataset_10M, open('../data/dataset_10M.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
